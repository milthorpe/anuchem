Pumjarasaayani Hartree-Fock code, part of ANU-Chem

This program is controlled by the Eclipse Public Licence v1.0 http://www.eclipse.org/legal/epl-v10.html

Introduction:
------------

Pumjarasaayani is a very basic quantum chemistry code written in X10 that performs 
Hartree-Fock (HF) single point energy (SPE) evaluation on a given molecular 
system using a specified basis set.

Current restrictions: Only closed shell systems with atoms H, C, N and O are supported.
No integral screening employed, all O(N^4/8) combinations evaluated. 
No geometrical symmetry is employed.
Convergence acceleration of HF - SCF procedure is applied automatically, but cannot be 
switched off. 

A preliminary framework for a divide and conquer algorithm (specifically the
Molecular Tailoring Approach - MTA) is also available with in Pumjarasaayani. However,
this is not fully implemented and will only work for simple chain style molecules.

Pumjarasaayani is free software under the Eclipse Public License.

Compiling:
----------

$ ant

will build the required executable and put it in bin/pumjarasaayani

$ ant clean

will remove the executable and all intermediate C++ files.

The default build script builds against the "sockets" implementation of X10RT.
To build against the MPI implementation set property ${x10rt} to "mpi".
For other implementations see 
  http://x10-lang.org/documentation/getting-started/x10rt-implementations.html

Dependency for compiling:
------------------------
* X10 version 2.2.1 (optionally with x10rt=mpi)
* GSL or equivalent BLAS library, will require rewrite of native wrappers for 
  any library other than GSL. However, the code can be compiled without any
  native wrappers. GSL is required for diagonalization and DIIS.
  http://www.gnu.org/software/gsl/
* anu-chem library
* x10x.lib library

Code organization:
-----------------
* QM code in: au.edu.anu.qm  (Pumjarasaayani.x10 is the main entry point)
* XLA code in: x10x.xla, x10x.matrix and x10x.vector
* GSL wrapper code in: org.gnu.gsl
* ESSL wrapper in: bgp (this is incomplete and not tested)
* anu-chem code in: au.edu.anu.chem, au.edu.anu.util 

To run:
------

The test cases are in directory: test/
The basis sets are in directory: test/basis/   [ contains sto-3g, 3-21g, and 6-31g(d,p) ]

Basis sets are assumed to be in Gaussian 94 format.
The provided basis sets were taken from EMSL Basis Set Exchange.
https://bse.pnl.gov/bse/portal

To test, you need to be in a directory above basis/

Single place:

$ runx10 bin/pumjarasaayani test/h2o.inp [<gmatype>] [-mta]

The -mta switch is used to invoke the molecular tailoring algorithm. This basically
splits the molecule into a series of overlapping fragments and then performs HF on 
individual fragments. The results are finally patched back to give total energy 
of the big molecule. 

Multiple place:

$ launcher -t <n> bin/pumjarasaayani test/h2o.inp [<gmatype>] [-mta]  { pgas compiled }
$ mpiexec -n <n> bin/pumjarasaayani test/h2o.inp [<gmatype>]  [-mta] { mpi compiled  }

where <n> is number of places

[<gmatype>] is:
0 : serial G matrix formation using a low flop count integral evaluation (but uses recursion)
1 : like (0) , but uses pseudocode (2) for multithreaded implementation
2 : (1) implemented as multi place code [*]
3 : (1) using purely static load balancing [*]
4 : (1) load balancing based on Bernholdt "Code 3", uses future and atomic [*] [**]
5 : like (3) but uses a shell based loop [*] (the default)

[*] - most relevant for timings and scaling at this point
[**] - currently segfaults for many test cases with multiple places

Note: options 3 and 5 do not have multithreaded implementation at present, so wont scale on a single place
For most performance tests, Option 5 is the best suitable at the moment. 

When -mta switch is active each fragment is run sequentially over multiple places. This is not 
optimal but works at the moment. 

Tests for which all the above run correctly
..................................
h2-sto.inp   h2.inp   h2o.inp   h2o-dp.inp  nh3.inp  nh3-dp.inp  ch4-dp.inp 
benzene.inp  benzene-321.inp benzene-dp.inp  h2co.inp h2co-321.inp
cyclohexane-321.inp aspirin.inp(*) aspirin-321.inp(*)

(*) Indicates it runs, but has convergence problems


A summary of what the code does:
-----------------------------------

The code solves the Hartree-Fock equations of the form:

   FC = eSC

which is a pseudo-eigenvalue problem, solved using a self-consistent
field (SCF) procedure. The most expensive step is the formation of the 
Fock (F) matrix, which requires computation of ~O(N^4) two electron integrals. 
In the code, the F matrix is in turn constructed using Hcore and G matrices. 
Of these the G matrix computation takes up most of the time and is implemented 
using X10 parallel constructs in following ways.

For a reference, see previous work by Bernholdt et al.:

"Programmability of the HPCS Languages: A Case Study with a Quantum
Chemistry Kernel", Bernholdt et al. HIPS'08.

1. Single place implementation
..............................

'nfunc' is the number of basis functions used to represent the molecule.
The compute (i, j, k, l) tasks are completely independent of each other. 

Pseudocode for reference:

Pseudocode (1): GMatrix.computeDirectAsyncOld(), GMatrix.computeDirectLowMemNewNoAtomic()

for(i=0; i<nfuncs; i++)
   for(j=0; j<nfuncs; j++)
      for(k=0; k<nfuncs; k++)
          for(l=0; l<nfuncs; l++)
              async computeAndUpdate(i, j, k, l); and set global G matrix elements, 
                                                  using atomic block

Pseudocode (2): GMatrix.computeDirectAsyncOldNoAtomic()
(a) 
  for(i=0; i<Runtime.INIT_THREADS; i++) {
            compute(i) = new ComputeTask(..);
  }
 
(b)
for(i=0; i<nfuncs; i++)
   for(j=0; j<nfuncs; j++)
      for(k=0; k<nfuncs; k++)
          for(l=0; l<nfuncs; l++) {
              selected = false;
              outer: while(!selected) {
                     for(var ix:Int=0; ix<Runtime.INIT_THREADS; ix++) {
                            if (selected = compute(ix).select(i, j, k, l)) { 
                               async compute(ix).doIt(); and set local G matrix elements
                               break outer; 
                            }
                     } 
          }

(c)
SUM all partial G matrices 

Currently, for option 5, (b) scales OK, but parts (a) and (c) scale negatively and needs
to be re-written. 

2. Multi place implementation
.............................

Use two level parallelism, the first level over places and the second over the 
number of threads as indicated above. [ GMatrix.computeDirectMultiPlaceNoAtomic() ]


Expected performance
-------------------
The computation of each (ij|kl) integral used to construct the G matrix is independent 
of the others. However, the time it take to compute an (ij|kl) will vary depending
on the angular momentum.  

For a given molecular system and basis set, with proper load balancing there should
be near-linear reduction in the time to construct the G matrix with increase 
in number of processors. Scaling will be worse if there are a relatively small number
of integrals to be evaluated, relative to the number of places, as this results
in a load imbalance between integrals of small and large angular momentum.
(TODO quantify)

The current implementation seem to have a bottleneck with the task scheduler 
approach (pseudocode 2 above), as time is spent in spinning to see which place 
is free to take up a task.

With Option 5  (shell-based loops) the scaling on larger number of processors
is reasonable for large problems. However, the time for job setup and G matrix
summation scale linearly with the number of places, so the overall scaling is
reduced.


Benchmark results
--------------

See doc/pr-perform-bgp.ods



