Pumjarasaayani Hartree-Fock code, part of ANU-Chem

This program is controlled by the Eclipse Public Licence v1.0 http://www.eclipse.org/legal/epl-v10.html

Introduction:
------------

Pumjarasaayani is a very basic quantum chemistry code written in X10 that performs Hartree-Fock (HF) single point energy (SPE) evaluation on a given molecular system using a specified basis set.

Current restrictions: Only closed shell systems with atoms H, C, N and O are supported. No geometrical symmetry is employed.
Convergence acceleration of HF - SCF procedure is applied automatically, but cannot be switched off. 
Integral calculation is inefficient as it is performed through a horizontal recurrence relation with no reuse of intermediate products.

Integral screening is applied by Schwartz bound as described in HÃ¤ser & Ahlrichs (1989).

A preliminary framework for a divide and conquer algorithm (specifically the Molecular Tailoring Approach - MTA) is also available within Pumjarasaayani. However, this is not fully implemented and will only work for simple chain style molecules.

Pumjarasaayani is free software under the Eclipse Public License.


Compiling:
----------

$ ant

will build the required executable and put it in bin/pumjarasaayani

$ ant clean

will remove the executable and all intermediate C++ files.

The default build script builds against the "sockets" implementation of X10RT.
To build against the MPI implementation set property ${x10rt} to "mpi".
For other implementations see 
  http://x10-lang.org/documentation/getting-started/x10rt-implementations.html

Compile dependencies:
------------------------
* X10 version 2.2.2 (optionally with x10rt=mpi)
* GSL or equivalent BLAS library, will require rewrite of native wrappers for any library other than GSL. However, the code can be compiled without any native wrappers. GSL is required for diagonalization and DIIS.
  http://www.gnu.org/software/gsl/
* anu-chem library (cd ~/x10-apps/anuchem && ant)
* XLA library (cd ~/x10-apps/x10x.lib/xla && ant)


Code organization:
-----------------
* QM code in: au.edu.anu.qm  (Pumjarasaayani.x10 is the main entry point)
* XLA code in: x10x.xla, x10x.matrix and x10x.vector
* GSL wrapper code in: org.gnu.gsl
* ESSL wrapper in: bgp (this is incomplete and not tested)
* anu-chem code in: au.edu.anu.chem, au.edu.anu.util 


To run:
------

The test cases are in directory: test/
The basis sets are in directory: test/basis/   [ contains sto3g, 321g, and 631gdp ]
Note: the basis set for a test case must be located in a subdirectory ./basis

Basis sets are assumed to be in Gaussian 94 format.
The provided basis sets were taken from EMSL Basis Set Exchange.
https://bse.pnl.gov/bse/portal

Single place:

$ runx10 bin/pumjarasaayani test/H2O-321g.inp

Multiple place:

$ launcher -t <n> bin/pumjarasaayani test/H2O-321g.inp { pgas compiled }
$ mpiexec -n <n> bin/pumjarasaayani test/H2O-321g.inp { mpi compiled  }

where <n> is number of places


Input file format:
-----------------------------------
  <title>
  basis <6-31gdp>
+ charge <n>
+ multiplicity <n> [if charge specified]
+ conversion <c> [optional unit conversion factor]
  molecule
  <symbol> <x> <y> <z>
  ...
  end
+ scf_method [diis|roothaan]
+ scf_max <n>
+ scf_converge <x>
+ diis_start <x>
+ diis_converge <x>
+ diis_subspace size <n>
+ gmat_parallel_scheme <n>
+ fragment mta
+ scratch <filename>
+ checkpoint <filename>
+ guess [core|sad|file]
* print [hcore|overla|orthonorm|2e] [<filename>]
* scf_print [mo|density] [<filename>]
* scf_final_print [mo|density] [<filename>]

Lines marked with "+" are optional; lines marked with "*" are optional and may be specified for multiple options.

== conversion ==

Coordinates are assumed to be in a.u. if no conversion factor is supplied.

== fragment mta ==

This invokes the molecular tailoring algorithm, which splits the molecule into a series of overlapping fragments and then performs HF on individual fragments. The results are finally patched together to give the total energy of the molecule. Each fragment is run sequentially over multiple places. This is not optimal; it is preferred to divide the fragments among places.

== gmat_parallel_scheme ==

This selects a load-balancing scheme for integral evaluation.
Some options follow the schemes proposed in Bernholdt et al. (see below).
0 : serial G matrix formation using a low flop count integral evaluation (but uses recursion)
1 : multi-threaded, dynamic load balancing as per pseudocode 2 below
2 : multi-place, dynamic load balancing similar to pseudocode 2 below
3 : multi-place, static load balancing by atom
4 : multi-place, dynamic load balancing by shell pair using shared counter (Bernholdt code 3) (the default) [*]
5 : multi-place, static load balancing by shell pair [*] 
6 : as per (5) with density matrix reduction using Team.allreduce
7 : multi-place dynamic load balancing by shell pair; multi-threaded dynamic load balancing between threads by shell triplet [*]

[*] - most relevant for timings and scaling at this point

Note: options 3, 4 and 5 are for single-threaded places and do not take advantage of threading within a place.
For clusters of SMP nodes, option 7 should provide best performance as it makes use of shared memory and cache hierarchies within a node.
For example, on a cluster of 32 Nehalem nodes with eight cores per node, run:
mpiexec -bind-to-node -n 32 -x X10_NTHREADS=8 -x X10_STATIC_THREADS=true bin/pumjarasaayani test/aspirin-321g.inp 7


Purpose and description:
-----------------------------------

The code solves the Hartree-Fock equations of the form:

   FC = eSC

which is a pseudo-eigenvalue problem, solved using a self-consistent field (SCF) procedure. The most expensive step is the formation of the Fock (F) matrix, which requires computation of ~O(N^4) two electron integrals. In the code, the F matrix is in turn constructed using Hcore and G matrices. Of these the G matrix computation takes up most of the time and is implemented using X10 parallel constructs in following ways.

For a reference, see previous work by Bernholdt et al.:

"Programmability of the HPCS Languages: A Case Study with a Quantum Chemistry Kernel", Bernholdt et al. HIPS'08.

1. Single place implementation
..............................

'nfunc' is the number of basis functions used to represent the molecule.
The compute (i, j, k, l) tasks are completely independent of each other. 

Pseudocode for reference:

Pseudocode (1): GMatrix.computeThreadedLowMemNoAtomicByBF()

for(i=0; i<nfuncs; i++)
   for(j=0; j<nfuncs; j++)
      for(k=0; k<nfuncs; k++)
          for(l=0; l<nfuncs; l++)
              async computeAndUpdate(i, j, k, l); and set global G matrix elements, 
                                                  using atomic block

Pseudocode (2): GMatrix.computeDistNoAtomicByBF()
(a) 
  zero partial G matrix contributions
  (multi-place) broadcast new density matrix to all places
(b)
for(i=0; i<nfuncs; i++)
   for(j=0; j<nfuncs; j++)
      for(k=0; k<nfuncs; k++)
          for(l=0; l<nfuncs; l++) {
              selected = false;
              outer: while(!selected) {
                     for(var ix:Int=0; ix<Runtime.INIT_THREADS; ix++) {
                            if (selected = compute(ix).select(i, j, k, l)) { 
                               async compute(ix).doIt(); and set local G matrix elements
                               break outer; 
                            }
                     } 
          }

(c)
SUM all partial G matrices 

For options 4, 5, and 7, part (a) uses a collective broadcast as so scales as the log of the number of places; part (b) is embarrassingly parallel, but part (c) scales linearly with the number of places.  Option 6 uses Team.allreduce, however this is currently "emulated" for x10rt=mpi, so scales poorly (linearly with the number of places).

2. Multi place implementation
.............................

Uses two level parallelism, the first level over places and the second over the number of threads as indicated above. [ GMatrix.computeDistNoAtomicByBF() ]


Expected performance
-------------------
The computation of each (ij|kl) integral used to construct the G matrix is independent of the others. However, the time taken to compute an (ij|kl) will vary depending on the angular momentum.

For a given molecular system and basis set, with proper load balancing there should be near-linear reduction in the time to construct the G matrix with increase in number of processors. Scaling will be worse if there are a relatively small number of integrals to be evaluated, relative to the number of places, as this results in a load imbalance between integrals of small and large angular momentum. (TODO quantify)

The current implementation seem to have a bottleneck with the task scheduler approach (pseudocode 2 above), as time is spent in spinning to see which place is free to take up a task.

With Option 5  (shell-based loops) the scaling on larger number of processors is reasonable for large problems. However, the time for job setup and G matrix summation scale linearly with the number of places, so the overall scaling is reduced.


Benchmark results
--------------

See doc/pr-perform-bgp.ods


Referencing Pumja Rasaayani:
-----------------------------

Please cite the following paper when referencing Pumja Rasaayani in a publication.

J. Milthorpe, V. Ganesh, A.P. Rendell, and D. Grove (2011). X10 as a parallel language for scientific computation: practice and experience, in proceedings of the 25th IEEE International Parallel & Distributed Processing Symposium (ISBN 978-0-7695-4385-7), 1067-1075
