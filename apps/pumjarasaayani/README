Pumjarasaayani Hartree-Fock code, part of ANU-Chem

This program is controlled by the Eclipse Public Licence v1.0
http://www.eclipse.org/legal/epl-v10.html

Introduction:
------------

Pumjarasaayani is a basic quantum chemistry code written in X10 that performs
Hartree-Fock (HF) single point energy (SPE) evaluation on a given molecular
system using a specified basis set.
Construction of the Fock matrix is via standard two-electron integral
calculation as described in McMurchie and Davidson (1978) or by Resolution of
the Operator as described by Limpanuparb (2012)[1] and Limpanuparb 
et al. (2014)[2].

Current restrictions:
- Only closed shell systems with atoms H, C, N and O are supported.
- No geometrical symmetry is employed.
- Convergence acceleration of HF - SCF procedure is applied automatically, but
  cannot be switched off. 
- Two-electron integral calculation is inefficient as it is performed through a
  horizontal recurrence relation with no reuse of intermediate products.

Integral screening is applied by Schwartz bound as described in HÃ¤ser & Ahlrichs (1989).

A preliminary framework for a divide and conquer algorithm (specifically the
Molecular Tailoring Approach - MTA) is also available within Pumjarasaayani.
However, this is not fully implemented and will only work for simple chain
style molecules.

[1] T. Limpanuparb (2012). "Applications of Resolutions of the Coulomb Operator
    in Quantum Chemistry", PhD Thesis, Australian National University.
[2] T. Limpanuparb, J. Milthorpe and A.P. Rendell (2014). "Resolutions of the
    Coulomb Operator: VIII. Parallel implementation using the modern
    programming language X10", Journal of Computational Chemistry. 2014 35 (28)
    2056-2069. doi:10.1002/jcc.23720

Compiling:
----------

$ ant

will build the required executable and put it in bin/pumjarasaayani

$ ant clean

will remove the executable and all intermediate C++ files.

The default build script builds against the "sockets" implementation of X10RT.
To build against the MPI implementation set property ${x10rt} to "mpi".
For other implementations see 
  http://x10-lang.org/documentation/getting-started/x10rt-implementations.html

Compile dependencies:
------------------------
* X10 version 2.5.1 (optionally with x10rt=mpi)
* X10 Global Matrix Library (distributed with X10 source under x10/x10.gml)
* LAPACK and BLAS libraries (transitive through X10 GML)
  http://www.netlib.org/lapack/
* XLA library (cd ~/anuchem-code/x10x.lib/xla && ant)
* anu-chem library (cd ~/anuchem-code/anuchem && ant)


Code organization:
-----------------
* QM code in: au.edu.anu.qm  (Pumjarasaayani.x10 is the main entry point)
* Optimized C++ code for Resolution of the Operator in ro
* molecular tailoring code in au.edu.anu.qm.mta
* anu-chem code in: au.edu.anu.chem, au.edu.anu.util
* XLA code in: x10x.vector

To run:
------

The test cases are in directory: test/

The basis sets are in directory: test/basis/ including:
- Minimal basis set STO-3G
- Pople basis sets e.g. 3-21G, 6-311G
- Correlation-consistent basis sets e.g. cc-pVQZ

Note: the basis set for a test case must be located in a subdirectory ./basis

Basis sets are assumed to be in Gaussian 94 format.
The provided basis sets were taken from EMSL Basis Set Exchange.
https://bse.pnl.gov/bse/portal

Single place:

$ bin/pumjarasaayani test/H2O-321g.inp

Multiple place:

$ launcher -t <n> bin/pumjarasaayani test/H2O-321g.inp { pgas compiled }
$ mpiexec -n <n> bin/pumjarasaayani test/H2O-321g.inp { mpi compiled  }

where <n> is number of places


Input file format:
-----------------------------------
  <title>
  basis <6-31gdp>
+ charge <n>
+ multiplicity <n> [if charge specified]
+ conversion <c> [optional unit conversion factor]
  molecule
  <symbol> <x> <y> <z>
  ...
  end
+ scf_method [diis|roothaan]
+ scf_max <n>
+ scf_converge <x>
+ diis_start <x>
+ diis_converge <x>
+ diis_subspace size <n>
+ gmat_parallel_scheme <n>
+ fragment mta
+ scratch <filename>
+ checkpoint <filename>
+ guess [core|sad|file]
* print [hcore|overla|orthonorm|2e] [<filename>]
* scf_print [mo|density] [<filename>]
* scf_final_print [mo|density] [<filename>]

Lines marked with "+" are optional; lines marked with "*" are optional and may
be specified for multiple options.

== conversion ==

Coordinates are assumed to be in a.u. if no conversion factor is supplied.

== fragment mta ==

This invokes the molecular tailoring algorithm, which splits the molecule into
a series of overlapping fragments and then performs HF on individual fragments.
The results are finally patched together to give the total energy of the
molecule. Each fragment is run sequentially over multiple places. This is not
optimal; it is preferred to divide the fragments among places.

== gmat_parallel_scheme ==

This selects a load-balancing scheme for integral evaluation.
Some options follow the schemes proposed in Bernholdt et al. (see below).
0 : serial G matrix formation using a low flop count integral evaluation (but
    uses recursion)
1 : multi-threaded, dynamic load balancing as per pseudocode 2 below
2 : multi-place, dynamic load balancing similar to pseudocode 2 below
3 : multi-place, static load balancing by atom
4 : multi-place, dynamic load balancing by shell pair using shared counter
    (Bernholdt code 3) (the default) [*]
5 : multi-place, static load balancing by shell pair [*] 
6 : as per (5) with density matrix reduction using Team.allreduce
7 : multi-place dynamic load balancing by shell pair; multi-threaded dynamic
    load balancing between threads by shell triplet [*]

[*] - most relevant for timings and scaling at this point

Note: options 3, 4 and 5 are for single-threaded places and do not take
advantage of threading within a place.
For clusters of SMP nodes, option 7 should provide best performance as it makes
use of shared memory and cache hierarchies within a node.
For example, on a cluster of 32 Nehalem nodes with eight cores per node, run:
mpiexec -bind-to-node -n 32 -x X10_NTHREADS=8 -x X10_STATIC_THREADS=true \
  bin/pumjarasaayani test/aspirin-321g.inp 7


Purpose and description:
-----------------------------------

The code solves the Hartree-Fock equations of the form:

   FC = eSC

which is a pseudo-eigenvalue problem, solved using a self-consistent field
(SCF) procedure. The most expensive step is the formation of the Fock (F)
matrix, which requires computation of ~O(N^4) two electron integrals. In the
code, the F matrix is in turn constructed using Hcore and G matrices. Of these,
the G matrix computation takes up most of the time and is implemented using X10
parallel constructs in following ways.

For a reference, see previous work by Bernholdt et al.:

"Programmability of the HPCS Languages: A Case Study with a Quantum Chemistry
Kernel", Bernholdt et al. HIPS'08.

1. Single place implementation
..............................

'nfunc' is the number of basis functions used to represent the molecule.
The compute (i, j, k, l) tasks are completely independent of each other. 

Pseudocode for reference:

Pseudocode (1): GMatrix.computeThreadedLowMemNoAtomicByBF()

for(i=0; i<nfuncs; i++)
   for(j=0; j<nfuncs; j++)
      for(k=0; k<nfuncs; k++)
          for(l=0; l<nfuncs; l++)
              async computeAndUpdate(i, j, k, l); and set global G matrix elements, 
                                                  using atomic block

Pseudocode (2): GMatrix.computeDistNoAtomicByBF()
(a) 
  zero partial G matrix contributions
  (multi-place) broadcast new density matrix to all places
(b)
for(i=0; i<nfuncs; i++)
   for(j=0; j<nfuncs; j++)
      for(k=0; k<nfuncs; k++)
          for(l=0; l<nfuncs; l++) {
              selected = false;
              outer: while(!selected) {
                     for(var ix:Int=0; ix<Runtime.INIT_THREADS; ix++) {
                            if (selected = compute(ix).select(i, j, k, l)) { 
                               async compute(ix).doIt(); and set local G matrix elements
                               break outer; 
                            }
                     } 
          }

(c)
SUM all partial G matrices 

For options 4, 5, and 7, part (a) uses a collective broadcast as so scales as
the log of the number of places; part (b) is embarrassingly parallel, but part
(c) scales linearly with the number of places.  Option 6 uses Team.allreduce,
however this is currently "emulated" for x10rt=mpi, so scales poorly (linearly
with the number of places).

2. Multi place implementation
.............................

Uses two level parallelism, the first level over places and the second over the
number of threads as indicated above. [ GMatrix.computeDistNoAtomicByBF() ]


Expected performance
-------------------
The computation of each (ij|kl) integral used to construct the G matrix is
independent of the others. However, the time taken to compute an (ij|kl) will
vary depending on the angular momentum.

For a given molecular system and basis set, with proper load balancing there
should be near-linear reduction in the time to construct the G matrix with
increase in number of processors. Scaling will be worse if there are a
relatively small number of integrals to be evaluated, relative to the number of
places, as this results in a load imbalance between integrals of small and
large angular momentum. (TODO quantify)

The current implementation seem to have a bottleneck with the task scheduler
approach (pseudocode 2 above), as time is spent in spinning to see which place
is free to take up a task.

With Option 5  (shell-based loops) the scaling on larger number of processors
is reasonable for large problems. However, the time for job setup and G matrix
summation scale linearly with the number of places, so the overall scaling is
reduced.


Referencing Pumja Rasaayani:
-----------------------------
Please cite the following paper when referencing Pumja Rasaayani in a
publication.

T. Limpanuparb, J. Milthorpe and A.P. Rendell (2014). "Resolutions of the
Coulomb Operator: VIII. Parallel implementation using the modern programming
language X10", Journal of Computational Chemistry. 2014 35 (28) 2056-2069.
doi:10.1002/jcc.23720

The following paper contains further details on the Resolution of the Operator
approach:

T. Limpanuparb, J. Milthorpe, A.P. Rendell, and P.M.W. Gill (2013).
"Resolutions of the Coulomb Operator: VII. Evaluation of Long-range Coulomb and
Exchange Matrices", Journal of Chemical Theory and Computation. 2013 9 (2)
863-867. doi:10.1021/ct301110y
