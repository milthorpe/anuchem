Introduction:
------------

Pumjarasaayani is a very basic quantum chemistry code written in X10 that performs 
Hartree-Fock (HF) single point energy (SPE) evaluation on a given molecular 
system using a specified basis set.

Current restrictions: Only closed shell systems with atoms H, C, N and O are supported.
No integral screening employed, all O(N^4/8) combinations evaluated. 
No geometrical symmetry is employed.
Convergence acceleration of HF - SCF procedure is applied automatically, but cannot be 
switched off. 

A preliminary framework for a divide and conqure algorithm (specifically the
Molecular Tailoring Approach - MTA) is also available with in Pumjarasaayani. However,
this is not fully implemented and will only work for simple chain style molecules.

Pumjarasaayani is free software under the Eclipse Public License.

Compiling:
----------

$ ant clean && ant

will build the required executable and put it in bin/pumjarasaayani.exe

The default build script builds agaist pgas_sockets.
To build against MPI change ${x10rt} to "mpi" and the 
${x10.postcompiler} to "mpixx"

Dependency for compiling:
------------------------
* GSL or equivalent BLAS library, will require rewrite of native wrappers for 
  any library other than GSL. However, the code can be compiled without any
  native wrappers. GSL is required for diagonalization and DIIS.
* anu-chem library
* x10x.lib library

Code organization:
-----------------
* QM code in: au.anu.edu.qm  (Pumjarasaayani.x10 is the main entry point)
* XLA code in: x10x.xla, x10x.matrix and x10x.vector
* GSL wrapper code in: org.gnu.gsl
* ESSL wrapper in: bgp (this is incomplete and not tested)
* anu-chem code in: au.edu.anu.chem, anu.edu.anu.util 

To run:
------

The test cases are in directory: test/
The basis set is in directory: basis/   [ contains sto-3g, 3-21g, and 6-31g(d,p) ]

To test, you need to be in a directory above basis/

Single place:

$ runx10 bin/pumjarasaayani.exe test/h2o.inp [<gmatype>] [-mta]

The -mta switch is used to invoke the molecular tailoring algorithm. This basically
splits the molecule into a series of overlapping fragments and then performs HF on 
individual fragments. The results are finally patched back to give total energy 
of the big molecule. 

Multiple place:

$ launcher -t <n> bin/pumjarasaayani.exe test/h2o.inp [<gmatype>] [-mta]  { pgas compiled }
$ mpirun -np <n> bin/pumjarasaayani.exe test/h2o.inp [<gmatype>]  [-mta] { mpi compiled  }

where <n> is number of places

[<gmatype>] is:
0 : serial G matrix formation using a low flop count integral evaluation (but uses recursion)
1 : like (0) , but uses pseudo code (2) for multithread implementation
2 : (1) implemented as multi place code [*]
3 : (1) using purely static load balancing [*]
4 : (1) load balancing based on Bernholdt "Code 3", uses future and atomic [*] [**]
5 : like (3) but uses a shell based loop [*]

[*] - most relevent for timings and scaling at this point
[**] - currently segfaults for many test cases with multiple places

Note: options 3 and 5 do not have multithreaded implementation at present, so wont scale on a single place
For most performance tests, Option 5 is the best suitable at the moment. 

When -mta switch is active each fragment is run sequentially over multiple places. This is not 
optimal but works at the moment. 

Tests for which all the above runs
..................................
h2-sto.inp   h2.inp   h2o.inp   h2o-dp.inp  nh3.inp  nh3-dp.inp  ch4-dp.inp 
benzene.inp  benzene-321.inp benzene-dp.inp(*)  h2co.inp(*) h2co-321.inp
aspirin.inp(*) aspirin-321.inp(*)

(*) Indicates it runs, but has convergence problems


A summary of what the code does:
-----------------------------------

The code solves the Hartree-Fock equations of the form:

   FC = eSC

which is a pseudo-eigenvalue problem, solved using a self-consistent
field (SCF) procedure. The most expensive step is the formation of the 
Fock (F) matrix, which requires computation of ~O(N^4) two electron integrals. 
In the code, the F matrix is in turn constructed using Hcore and G matrices. 
Of these the G matrix computation takes up most of the time and is implemented 
using X10 parallel constructs in following ways.

For a reference, see previous work by Bernholdt et al.:

"Programmability of the HPCS Languages: A Case Study with a Quantum
Chemistry Kernel", Bernholdt et al. HIPS'08.

1. Single place implementation
..............................

'nfunc' is the number of basis functions used to represent the molecule.
The compute (i, j, k, l) tasks are completely independent of each other. 

Pseudo code for reference:

Pseudo code (1): GMatrix.computeDirectAsyncOld(), GMatrix.computeDirectLowMemNewNoAtomic()

for(i=0; i<nfuncs; i++)
   for(j=0; j<nfuncs; j++)
      for(k=0; k<nfuncs; k++)
          for(l=0; l<nfuncs; l++)
              async computeAndUpdate(i, j, k, l); and set global G matrix elements, 
                                                  using atomic block

Pseudo code (2): GMatrix.computeDirectAsyncOldNoAtomic()
(a) 
  for(i=0; i<Runtime.INIT_THREADS; i++) {
            compute(i) = new ComputeTask(..);
  }
 
(b)
for(i=0; i<nfuncs; i++)
   for(j=0; j<nfuncs; j++)
      for(k=0; k<nfuncs; k++)
          for(l=0; l<nfuncs; l++) {
              selected = false;
              outer: while(!selected) {
                     for(var ix:Int=0; ix<Runtime.INIT_THREADS; ix++) {
                            if (selected = compute(ix).select(i, j, k, l)) { 
                               async compute(ix).doIt(); and set local G matrix elements
                               break outer; 
                            }
                     } 
          }

(c)
SUM all partial G matrices 

Currently, for option 5, (b) scales OK, but parts (a) and (c) scale negatively and needs
to be re-written. 

2. Multi place implementation
.............................

Use two level parallelism, the first level over places and the second over the 
number of threads as indicated above. [ GMatrix.computeDirectOldMultiPlaceNoAtomic(),
                                        GMatrix.computeDirectNewMultiPlaceNoAtomic() ]


Expected performace
-------------------
The computation of each (ij|kl) integral used to construct the G matrix is independent 
of the others. However, the time it take to compute an (ij|kl) will vary depending
on the angular momemtum.  

For a given molecular system and basis set, with proper load balancing there should
be near-linear scaling in the time required to construct the G matrix with increase 
in number of processors. Scaling will be worse if there are a relatively small number
of integrals to be evaluated, relative to the number of places. (TODO quantify)

The current implementation seem to have a bottleneck with the task scheduler 
approach (Pseudo code 2 above), as time is spent in spinning to see which place 
is free to take up a task.

With Option 11; the shell based loops; the scaling on larger number of processors
is reasonable. However the job set up cost and the time for GMatrix summation 
scales negatively with current implementation.


Few benchmarks
--------------

See doc/pr-perform-bgp.ods



